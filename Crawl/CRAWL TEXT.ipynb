{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0774c152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# ================== SETUP DRIVER ==================\n",
    "\n",
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "    )\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "# ================== C√ÄO 1 B√ÄI THEO URL ==================\n",
    "\n",
    "def crawl_article(driver, url):\n",
    "    data = {\n",
    "        \"url\": url,\n",
    "        \"title\": \"\",\n",
    "        \"author\": \"\",\n",
    "        \"publish_date\": \"\",\n",
    "        \"content\": \"\",\n",
    "        \"status\": \"success\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "\n",
    "        # ---- TI√äU ƒê·ªÄ ----\n",
    "        try:\n",
    "            title = wait.until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (By.CSS_SELECTOR, \"h1.title-detail, h1.title_news_detail, h1\")\n",
    "                )\n",
    "            )\n",
    "            data[\"title\"] = title.text.strip()\n",
    "        except:\n",
    "            data[\"title\"] = \"N/A\"\n",
    "\n",
    "        # ---- T√ÅC GI·∫¢ ----\n",
    "        try:\n",
    "            author = driver.find_element(\n",
    "                By.CSS_SELECTOR, \"p.author_mail strong, p.author strong\"\n",
    "            )\n",
    "            data[\"author\"] = author.text.strip()\n",
    "        except:\n",
    "            data[\"author\"] = \"VnExpress\"\n",
    "\n",
    "        # ---- NG√ÄY ƒêƒÇNG ----\n",
    "        try:\n",
    "            date = driver.find_element(By.CSS_SELECTOR, \"span.date, span.time\")\n",
    "            data[\"publish_date\"] = date.text.strip()\n",
    "        except:\n",
    "            data[\"publish_date\"] = \"N/A\"\n",
    "\n",
    "        # ---- N·ªòI DUNG ----\n",
    "        paragraphs = driver.find_elements(\n",
    "            By.CSS_SELECTOR, \"article.fck_detail p.Normal, div.fck_detail p\"\n",
    "        )\n",
    "        data[\"content\"] = \"\\n\".join(\n",
    "            p.text.strip() for p in paragraphs if p.text.strip()\n",
    "        )\n",
    "\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        data[\"status\"] = \"failed\"\n",
    "        data[\"error\"] = str(e)\n",
    "        return data\n",
    "\n",
    "# ================== CH·∫†Y THEO FILE JSON ==================\n",
    "\n",
    "def crawl_from_existing_urls(\n",
    "    input_file=r\"C:\\Documents\\UEH\\HKC 2025\\vnexpress_articles.json\",\n",
    "    output_dir=r\"C:\\Documents\\UEH\\HKC 2025\"\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # ƒê·ªçc file URL c√≥ s·∫µn\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        articles_by_category = json.load(f)\n",
    "\n",
    "    driver = setup_driver()\n",
    "    results = []\n",
    "\n",
    "    try:\n",
    "        for category, articles in articles_by_category.items():\n",
    "            print(f\"\\nüì∞ {category}\")\n",
    "\n",
    "            for i, article in enumerate(articles, 1):\n",
    "                url = article[\"url\"]\n",
    "                print(f\"  [{i}/{len(articles)}] {url}\")\n",
    "\n",
    "                data = crawl_article(driver, url)\n",
    "                data[\"category\"] = category\n",
    "                results.append(data)\n",
    "\n",
    "                time.sleep(2)\n",
    "\n",
    "        # L∆∞u JSON k·∫øt qu·∫£\n",
    "        out_json = os.path.join(output_dir, \"vnexpress_fulltext_from_urls.json\")\n",
    "        with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # L∆∞u CSV\n",
    "        out_csv = os.path.join(output_dir, \"vnexpress_fulltext_from_urls.csv\")\n",
    "        with open(out_csv, \"w\", encoding=\"utf-8-sig\") as f:\n",
    "            f.write(\"category,title,author,publish_date,content,url,status\\n\")\n",
    "            for a in results:\n",
    "                f.write(\n",
    "                    f'\"{a[\"category\"]}\",\"{a[\"title\"].replace(chr(34),\"\")}\",\"{a[\"author\"]}\",'\n",
    "                    f'\"{a[\"publish_date\"]}\",\"{a[\"content\"].replace(chr(34),\"\")}\",'\n",
    "                    f'\"{a[\"url\"]}\",\"{a[\"status\"]}\"\\n'\n",
    "                )\n",
    "\n",
    "        print(\"\\n‚úÖ HO√ÄN T·∫§T\")\n",
    "        print(f\"üìÑ JSON: {out_json}\")\n",
    "        print(f\"üìä CSV : {out_csv}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# ================== RUN ==================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_from_existing_urls()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
